## Methodology: Transformer-Enhanced LDA Topic Extraction

### 6.3 Topic Extraction Method 2: Transformer-Enhanced TF-IDF + LDA

The second topic extraction approach implements a hybrid methodology that combines the probabilistic foundations of TF-IDF and Latent Dirichlet Allocation with modern transformer-based semantic understanding. This approach addresses fundamental limitations of traditional LDA when applied to short-form blog content, particularly the bag-of-words assumption that ignores semantic relationships between terms and the challenge of maintaining topic coherence in sparse text environments.

The implementation leverages the Sentence-BERT framework [reimers2019sentence] through the `all-mpnet-base-v2` model, which provides dense vector representations that capture semantic meaning beyond surface-level word co-occurrence patterns. This transformer model was specifically chosen for its balanced performance across semantic similarity tasks and its ability to generate meaningful embeddings for both individual terms and longer text segments. The `all-mpnet-base-v2` architecture combines the benefits of masked language modeling with next sentence prediction, making it particularly well-suited for understanding contextual relationships within blog content where informal language and domain-specific terminology are prevalent.

### 6.3.1 Semantic Document Segmentation Strategy

Traditional topic modeling approaches often treat entire documents as atomic units, which can be problematic for blog content that may contain multiple distinct topics or conversational threads. The implemented approach addresses this through semantic document segmentation using spaCy's sentence boundary detection [?]. Our methodology employs a two-stage strategy that first attempts to create coherent semantic units by grouping 2-3 sentences together, ensuring each resulting document contains at least 20 characters of meaningful content.

This segmentation strategy is grounded in the principle that topic coherence improves when documents represent semantically cohesive units rather than arbitrary text chunks. When insufficient sentence-based documents are generated (fewer than 3), the implementation falls back to a token-based windowing approach using a sliding window of 30 tokens with 10-token overlap. This hybrid strategy ensures adequate document granularity while maintaining semantic coherence, a critical factor for effective topic modeling in blog environments where posts may be brief or contain multiple conversational turns.

### 6.3.2 Phrase Enhancement and Vocabulary Enrichment

The methodology incorporates phrase detection using Gensim's Phrases and Phraser implementations [rehurek2010software] to capture multi-word expressions that function as semantic units. The phrase detection employs conservative parameters with minimum count thresholds of 2 for bigrams and 1 for trigrams, combined with scoring thresholds of 8 and 10 respectively. These parameters were calibrated to balance phrase quality with vocabulary coverage, ensuring that meaningful compound terms like "high school" or "video games" are preserved as single tokens rather than being decomposed into individual words that may lose semantic specificity.

The phrase enhancement process utilizes a two-stage approach where bigram detection precedes trigram detection, allowing the system to build progressively complex phrase structures. This sequential processing ensures that fundamental two-word phrases are established before attempting to identify longer expressions, reducing the risk of fragmented phrase detection that could compromise topic coherence.

### 6.3.3 Adaptive Topic Number Optimization

Unlike traditional fixed-parameter approaches, the implementation employs adaptive topic number optimization using silhouette analysis [?] on document embeddings generated by the transformer model. The optimize_topic_number method evaluates clustering quality for topic numbers ranging from 2 to a maximum of 6, using K-means clustering on the semantic embedding space to approximate the natural topic structure within the content.

The silhouette coefficient provides a measure of how well-separated the resulting topic clusters are, with values closer to 1 indicating well-defined topic boundaries. This approach leverages the semantic understanding provided by transformer embeddings to inform the probabilistic topic modeling process, creating a bridge between geometric clustering in embedding space and the generative assumptions of LDA. The maximum topic limit of 6 was established based on the typical cognitive limits for topic comprehension and the practical constraints of blog-length content analysis.

### 6.3.4 TF-IDF Vectorization with Enhanced Parameters

The document-term matrix construction employs TF-IDF vectorization with parameters specifically optimized for short-form blog content. [laureate2023systematic] The implementation uses a maximum feature limit of 1000 terms, which provides sufficient vocabulary richness for topic discrimination while maintaining computational efficiency. The n-gram range of (1,3) captures both individual terms and meaningful phrase structures, while the minimum document frequency threshold of 2 ensures that terms appear in at least two semantic document segments before being considered for topic modeling.

The sublinear term frequency scaling (sublinear_tf=True) applies logarithmic normalization to term frequencies, reducing the impact of extremely frequent terms that might dominate topic distributions. This is particularly important in blog content where certain common expressions or filler words might appear disproportionately. The smooth inverse document frequency parameter (smooth_idf=True) prevents zero divisions in IDF calculations while ensuring that the IDF component remains numerically stable across varying document collection sizes.

### 6.3.5 LDA Training with Online Learning

The core topic modeling employs scikit-learn's LatentDirichletAllocation implementation [pedregosa2011scikit] configured for online learning rather than batch processing. The online learning method utilizes mini-batch updates that process subsets of documents iteratively, making it computationally more efficient for larger datasets while maintaining convergence properties. This approach is particularly beneficial for the blog analysis pipeline where processing efficiency directly impacts scalability.

The hyperparameter configuration includes document-topic prior (alpha) and topic-word prior (beta) values of 0.1 and 0.01 respectively. These low prior values encourage sparse distributions, which is appropriate for blog content where individual posts typically focus on a limited number of topics and use relatively specific vocabulary. The learning offset parameter of 50.0 controls the learning rate decay, ensuring that early iterations have sufficient influence on the final model while allowing for stable convergence.

### 6.3.6 Semantic Re-ranking and Topic Refinement

A critical innovation in this approach involves post-processing the raw LDA output through semantic re-ranking using transformer embeddings. Traditional LDA topic-word distributions are based purely on co-occurrence statistics, which may not align with semantic coherence as understood by human interpreters. The implementation addresses this by computing semantic centroids for each topic using the transformer embeddings of the top-weighted words.

For each topic, the method calculates a centroid vector by averaging the embeddings of the eight highest-probability words according to LDA. Subsequently, all candidate topic words are re-ranked based on their cosine similarity to this semantic centroid. This process ensures that the final topic word lists maintain both statistical significance (from LDA) and semantic coherence (from transformer understanding).

The similarity threshold of 0.25 for word inclusion represents a balance between semantic coherence and vocabulary diversity. Words exceeding this threshold are considered semantically related to the topic core, while a higher threshold of 0.35 provides an additional quality gate for particularly coherent terms. This dual-threshold approach allows for flexibility in topic composition while maintaining semantic integrity.

### 6.3.7 Topic Quality Assessment and Filtering

The implementation incorporates a comprehensive topic quality scoring mechanism that combines multiple evaluation dimensions. The Topic Quality process integrates weighted semantic coherence (50% weight), word distinctiveness (30% weight), and vocabulary diversity (20% weight) into a composite quality score ranging from 0 to 1.

Semantic coherence is computed using the similarity scores between topic words and the semantic centroid, with higher weights given to the top-ranking words to emphasize topic core strength. Word distinctiveness is measured through the entropy of the TF-IDF weight distribution among top words, where lower entropy indicates more distinctive term distributions. Vocabulary diversity assesses the morphological variety within topic words by examining unique word stems, encouraging topics that span related but distinct semantic concepts.

The filtering process applies adaptive quality thresholds based on the distribution of scores across all extracted topics. When the maximum quality score exceeds 0.3, the threshold is set to 70% of this maximum; otherwise, it defaults to the greater of 80% of the maximum or the mean quality score. This adaptive approach ensures that filtering standards adjust to the overall quality of extractable topics within each document set, preventing overly restrictive filtering that might eliminate meaningful but imperfect topics.

### 6.3.8 Integration with Analysis Pipeline

The Topic Extraction process serves as the primary interface for integration with the broader analysis pipeline, returning structured results that include detailed topic information, consolidated word sets, and topic labels suitable for downstream processing. The method handles edge cases such as insufficient content gracefully, providing empty result sets rather than failing, which ensures pipeline stability across varying content quality levels.

The output structure preserves both the detailed topic modeling results needed for analysis and the simplified word and label sets required for demographic comparison tasks. This dual-output approach supports both in-depth topic analysis and high-level trend identification across the demographic segments specified in the assignment requirements.